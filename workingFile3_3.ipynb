{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project \"Training a cat to catch a Mouse using Q-learning and Qiskit\"\n",
    "\n",
    "Participants:\n",
    "\n",
    "## 1. Intoduction \n",
    "### 1.1 Idea\n",
    "The idea of this project is to train a novice(single agent) to catch a mouse in a grid environment 3*3. \n",
    "\n",
    "We placed the cat in the lower right corner and the mouse in the upper left corner. Our task is to find the best way for the cat to catch a motionless (and unsuspecting danger) mouse. We want to test whether it is possible to solve such a problem using quantum computing.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <th>MOUSE</th>\n",
    "    <td>EMPTY</td>\n",
    "    <td>EMPTY</td> \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td>EMPTY</td> \n",
    "    <td>EMPTY</td> \n",
    "    <td>EMPTY</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "    <td>EMPTY</td> \n",
    "    <td>EMPTY</td> \n",
    "    <th>CAT</th>   \n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "\n",
    "### 1.2 Q-Learning\n",
    "Q-Learning is a reinforcement learning algorithm to learn the value of an action in a particular state.\n",
    "When Q-learning is performed we create what’s called a **q-table** or matrix that follows the shape of *[state, action]* and we initialize our values to zero. We then update and store our q-values after an episode. This q-table becomes a reference table for our agent to select the best action based on the q-value.\n",
    "\n",
    "\n",
    "An agent interacts with the environment in 1 of 2 ways. The first is to use the q-table as a reference and view all possible actions for a given state. The agent then selects the action based on the max value of those actions. This is known as **exploiting** since we use the information we have available to us to make a decision.\n",
    "\n",
    "\n",
    "The second way to take action is to act randomly. This is called **exploring**. Instead of selecting actions based on the max future reward we select an action at random. Acting randomly is important because it allows the agent to explore and discover new states that otherwise may not be selected during the exploitation process. You can balance exploration/exploitation using epsilon (ε) and setting the value of how often you want to explore vs exploit. Here’s some rough code that will depend on how the state and action space are setup.\n",
    "\n",
    "Q-table is updated in every episode of training via **Bellman Equation**:\n",
    "\n",
    "\\begin{align}\n",
    "\\underbrace{\\text{New}Q(s,a)}_{\\scriptstyle\\text{New Q-Value}}=Q(s,a)+\\mkern-34mu\\underset{\\text{New Q-Value}}{\\underset{\\Bigl|}{\\alpha}}\\mkern-30mu \\underbrace{R(s,a)}_{\\scriptstyle\\text{Reward}}+ \\mkern-30mu\\underset{\\text{Discount rate}}{\\underset{\\Biggl |}{\\gamma}}\\mkern-75mu * \\overbrace{\\max Q'(s',a')}^{\\scriptstyle\\substack{\\text{Maximum predicted reward, given} \\\\ \\text{new state and all possible actions}}} * \\mkern-45mu-Q(s,a)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### 1.3 Variational Quantum Eigensolver (VQE)\n",
    "\n",
    "VQE is an application of the variational method of quantum mechanics.\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda_{min} \\le \\langle H \\rangle_{\\psi} = \\langle \\psi | H | \\psi \\rangle = \\sum_{i = 1}^{N} \\lambda_i | \\langle \\psi_i | \\psi\\rangle |^2\n",
    "\\end{align}\n",
    "\n",
    "The above equation is known as the **variational method**.\n",
    "\n",
    "When the Hamiltonian of a system is described by the Hermitian matrix $H$ the ground state energy of that system, $E_{gs}$, is the smallest eigenvalue associated with $H$. By arbitrarily selecting a wave function $|\\psi \\rangle$ (called an *ansatz*) as an initial guess approximating $|\\psi_{min}\\rangle$, calculating its expectation value, $\\langle H \\rangle_{\\psi}$, and iteratively updating the wave function, arbitrarily tight bounds on the ground state energy of a Hamiltonian may be obtained. \n",
    "\n",
    "A systematic approach to varying the ansatz is required to implement the variational method on a quantum computer. VQE does so through the use of a parameterized circuit with a fixed form. Such a circuit is often called a *variational form*, and its action may be represented by the linear transformation $U(\\theta)$. A variational form is applied to a starting state $|\\psi\\rangle$ (such as the vacuum state $|0\\rangle$, or the Hartree Fock state) and generates an output state $U(\\theta)|\\psi\\rangle\\equiv |\\psi(\\theta)\\rangle$. Iterative optimization over $|\\psi(\\theta)\\rangle$ aims to yield an expectation value $\\langle \\psi(\\theta)|H|\\psi(\\theta)\\rangle \\approx E_{gs} \\equiv \\lambda_{min}$. Ideally, $|\\psi(\\theta)\\rangle$ will be close to $|\\psi_{min}\\rangle$ (where 'closeness' is characterized by either state fidelity, or Manhattan distance) although in practice, useful bounds on $E_{gs}$ can be obtained even if this is not the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Solution scheme via Q-learning & Quantum Calculations\n",
    "\n",
    "The scheme of our solution is as follows: Each point of our medium of size 3*3 corresponds to a quantum circuit with configurable parameters. Each such scheme consists of several U3-gates, the type of which is shown below.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    U3(\\theta, \\phi, \\lambda) = \\begin{pmatrix}\\cos(\\frac{\\theta}{2}) & -e^{i\\lambda}\\sin(\\frac{\\theta}{2}) \\\\ e^{i\\phi}\\sin(\\frac{\\theta}{2}) & e^{i\\lambda + i\\phi}\\cos(\\frac{\\theta}{2}) \\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "One of the options for implementing such a quantum circuit looks like this:\n",
    "\n",
    "![Image of the Circuit](https://files.fm/thumb_show.php?i=9mynvq8t9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "from qiskit import Aer, transpile, assemble\n",
    "from qiskit.providers import backend\n",
    "from qiskit.aqua.components.optimizers import COBYLA\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPES:\n",
    "CAT = \"c\"\n",
    "# DOG = \"d\"\n",
    "MOUSE = \"m\"\n",
    "EMPTY = \"emp\"\n",
    "\n",
    "# ACTIONS:\n",
    "UP = \"00\"\n",
    "DOWN = \"01\"\n",
    "LEFT = \"10\"\n",
    "RIGHT = \"11\"\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "\n",
    "# random seed\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state of cat\n",
    "class State:\n",
    "    def __init__(self, catP):\n",
    "        #self.row = catP[0]\n",
    "        #self.column = catP[1]\n",
    "        self.catP = catP\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.catP == other.catP\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.catP))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State(cat_pos={self.catP})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld\n",
    "# e.g.\n",
    "#  MOUSE | EMPTY | EMPTY\n",
    "#  EMPTY | EMPTY | EMPTY\n",
    "#  EMPTY | EMPTY | CAT\n",
    "class GridWorld:\n",
    "    def __init__(self, s, catP, mouseP):\n",
    "        self.numRows = s[0]\n",
    "        self.numColumns = s[1]\n",
    "        self.catP = catP\n",
    "        self.mouseP = mouseP\n",
    "        # self.dogP = dogP\n",
    "        assert(not self.compaireList(self.catP, self.mouseP))\n",
    "    \n",
    "    def getItem(self, p):\n",
    "        if p[0]>=self.numRows or p[0]<0:\n",
    "            return None\n",
    "        if p[1]>=self.numColumns or p[1]<0:\n",
    "            return None\n",
    "        if self.compaireList(p, catP):\n",
    "            return CAT\n",
    "        elif self.compaireList(p, mouseP):\n",
    "            return MOUSE\n",
    "        # elif self.compaireList(p, DOG):\n",
    "        #     return DOG\n",
    "        else:\n",
    "            return EMPTY\n",
    "\n",
    "    def compaireList(self, l1,l2):\n",
    "        for i, j in zip(l1, l2):\n",
    "            if i!=j:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def getNumRows(self):\n",
    "        return self.numRows\n",
    "\n",
    "    def getNumColumns(self):\n",
    "        return self.numColumns\n",
    "\n",
    "    def getMouse(self):\n",
    "        return self.mouse\n",
    "    \n",
    "    def getCatP(self):\n",
    "        return self.catP\n",
    "\n",
    "    def setCatP(self, p):\n",
    "        self.catP = p\n",
    "        \n",
    "    def setMouseP(self, p):\n",
    "        self.mouseP = p\n",
    "    \n",
    "    def initCatState(self, rd = False):\n",
    "        # init cat position\n",
    "        if not rd:\n",
    "            catP = [self.getNumRows() - 1, self.getNumColumns() - 1]\n",
    "        else:\n",
    "            catP = [random.randint(0, self.getNumRows()), random.randint(0, self.getNumColumns())]\n",
    "            while self.getItem(catP) != EMPTY and self.getItem(catP) != CAT:\n",
    "                catP = [random.randint(0, self.getNumRows()), random.randint(0, self.getNumColumns())]\n",
    "        self.setCatP(catP)\n",
    "        return State(catP)\n",
    "    \n",
    "    def show(self):\n",
    "        output = \"\"\n",
    "        for i in range(self.numRows):\n",
    "            for j in range(self.numColumns):\n",
    "                if self.compaireList([i,j], self.catP):\n",
    "                    output += CAT + \" \"\n",
    "                if self.compaireList([i,j], self.mouseP):\n",
    "                    output += MOUSE + \" \"\n",
    "                if not self.compaireList([i,j], self.catP) and not self.compaireList([i,j], self.mouseP):\n",
    "                    output += EMPTY + \" \"\n",
    "            output += \"\\n\"\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNet\n",
    "class QNet:\n",
    "    \n",
    "    def __init__(self, qTable, gridWorld:GridWorld, alpha=0.1, gamma=1.0, eps=0.2, actions=[UP, DOWN, LEFT, RIGHT], numParams=6):\n",
    "        self.gw = gridWorld\n",
    "        self.qt = qTable\n",
    "        self.eps = eps\n",
    "        self.backend = Aer.get_backend(\"qasm_simulator\")\n",
    "        self.NUM_SHOTS = 1000 # number of measurements \n",
    "        self.optimizer = COBYLA(maxiter=500, tol=0.0001) # off the shelf\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.ACTIONS = actions\n",
    "\n",
    "        # self.rets = {(0,0):([0,..,0],0.0,0), ...}\n",
    "        self.rets = dict() # resulting parameters after optimization for all points in the grid\n",
    "\n",
    "        self.state = None\n",
    "        \n",
    "        for i in range(self.gw.getNumRows()):\n",
    "            for j in range(self.gw.getNumColumns()):\n",
    "                self.rets[i, j] = (np.random.rand(numParams), 0.0, 0) \n",
    "    \n",
    "    def qcMaker(self, params):\n",
    "        qr = QuantumRegister(2, name=\"q\")\n",
    "        cr = ClassicalRegister(2, name=\"c\")\n",
    "        qc = QuantumCircuit(qr, cr)\n",
    "        qc.u3(params[0], params[1], params[2], qr[0])\n",
    "        qc.u3(params[3], params[4], params[5], qr[1])\n",
    "        # qc.cx(qr[0], qr[1])\n",
    "        qc.measure(qr, cr)\n",
    "        return qc\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "        p = deepcopy(state.catP)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(self.gw.getNumRows() - 1, p[0]+1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(self.gw.getNumColumns() - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unkown action {action}\")\n",
    "        return p\n",
    "        \n",
    "    def getReward(self, p):\n",
    "        grid = self.gw.getItem(p)\n",
    "        if grid == EMPTY:\n",
    "            reward = -1\n",
    "        # elif grid == DOG:\n",
    "        #     reward = -1000\n",
    "        elif grid == MOUSE:\n",
    "            reward = 1000\n",
    "        elif grid == CAT:\n",
    "            reward = -1 # (maybe less than reward of empty)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward\n",
    "    \n",
    "    def selectAction(self, state, training):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            return random.choice(self.ACTIONS)\n",
    "        else:\n",
    "            if training:\n",
    "                self.state = deepcopy(state)\n",
    "                self.updateCircuit(state)\n",
    "            return self.ACTIONS[np.argmax(self.qt[self.state.catP[0], self.state.catP[1]])]\n",
    "        \n",
    "    def lossFunction(self, params):\n",
    "        action = \"\"\n",
    "        qc = self.qcMaker(params=params)\n",
    "        t_qc = transpile(qc, self.backend)\n",
    "        job = assemble(t_qc, shots=self.NUM_SHOTS)\n",
    "        rlt = self.backend.run(job).result()\n",
    "        counts = rlt.get_counts(qc)\n",
    "        # speedup training, cross the ravine\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            action = random.choice(self.ACTIONS)\n",
    "        else:\n",
    "            action = max(counts, key = counts.get)\n",
    "        \n",
    "        nextPosition = self.newPosition(self.state, action) # handle the \n",
    "        reward = self.getReward(nextPosition)\n",
    "        # update q-table(but not very sure, update only for this action or for all actions)\n",
    "        targetQvalue = reward + self.gamma *  np.max(self.qt[nextPosition[0],nextPosition[1]])\n",
    "        predictedQvalue = self.calculateQvalue(action, nextPosition, reward, targetQvalue, self.state)\n",
    "        \n",
    "        # update q-table\n",
    "        self.updateQtable(predictedQvalue, action)\n",
    "        return targetQvalue - self.qt[self.state.catP[0],self.state.catP[1]][int(action,2)]\n",
    "    \n",
    "    def updateQtable(self, predictedQvalue, action):\n",
    "        if self.qt[(self.state.catP[0],self.state.catP[1])][int(action,2)] < predictedQvalue:\n",
    "            self.qt[self.state.catP[0],self.state.catP[1]][int(action,2)] = predictedQvalue\n",
    "\n",
    "    def calculateQvalue(self, action, nextPosition, reward, targetQvalue, state:State):\n",
    "        targetQvalue = reward + self.gamma *  np.max(self.qt[nextPosition[0],nextPosition[1]])\n",
    "        return self.qt[state.catP[0], state.catP[1]][int(action,2)] + self.alpha * (targetQvalue - self.qt[state.catP[0],state.catP[1]][int(action,2)]) # update q-table\n",
    "\n",
    "    def updateCircuit(self, state:State):\n",
    "        self.rets[state.catP[0], state.catP[1]] = self.optimizer.optimize(num_vars=6, objective_function=self.lossFunction, initial_point=self.rets[state.catP[0], state.catP[1]][0])\n",
    "\n",
    "    def setAlpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def drawVectors(self, hasdiagonals):\n",
    "        # Draw vectors representing the cat's desired direction for each place in the grid based on the Qtable\n",
    "        x = np.linspace(0, self.gw.getNumColumns()-1, self.gw.getNumColumns())\n",
    "        y = np.linspace(0, self.gw.getNumColumns()-1, self.gw.getNumColumns())\n",
    "        pts = itertools.product(x, y)\n",
    "        plt.scatter(*zip(*pts), marker='o', s=30, color='red')\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        vecx = self.qt[(X, Y)][3]-self.qt[(X, Y)][2]\n",
    "        vecy = self.qt[(X, Y)][0]-self.qt[(X, Y)][1]\n",
    "        norm = vecx**2 + vecy**2\n",
    "        QP = plt.quiver(X, Y, vecx/norm, vecy/norm)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent: cat\n",
    "class Cat:\n",
    "    def __init__(self, qNet: QNet, training=True, eps = 0.2, actions = [UP, DOWN, LEFT, RIGHT]):\n",
    "        self.eps = eps\n",
    "        self.training = training\n",
    "        self.qNet = qNet\n",
    "        self.ACTIONS = actions\n",
    "        self.state = None\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "            p = deepcopy(state.catP)\n",
    "            if action == UP:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "            elif action == DOWN:\n",
    "                p[0] = min(self.qNet.gw.getNumRows() - 1, p[0] + 1)\n",
    "            elif action == LEFT:\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == RIGHT:\n",
    "                p[1] = min(self.qNet.gw.getNumColumns() - 1, p[1] + 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unkown action {self.ACTIONS[action]}\")\n",
    "            return p\n",
    "\n",
    "    def getReward(self, p):\n",
    "        grid = self.qNet.gw.getItem(p)\n",
    "        if grid == MOUSE:\n",
    "            reward = 1000\n",
    "            end = True\n",
    "        # elif grid == DOG:\n",
    "        #     reward = -100\n",
    "        #     end = True\n",
    "        #     self.qNet.gw.setCatP(p)\n",
    "        elif grid == EMPTY:\n",
    "            reward = -1\n",
    "            end = False\n",
    "        elif grid == CAT:\n",
    "            reward = -1 # (maybe less than reward of empty)\n",
    "            end = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward, end\n",
    "\n",
    "    def act(self, state, action):\n",
    "        p = self.newPosition(state, action)\n",
    "        reward, end = self.getReward(p)\n",
    "        return p, reward, end\n",
    "    \n",
    "    def updateQtable(self, action, p, reward, state):\n",
    "        pqv = self.qNet.calculateQvalue(action, p, reward, state)\n",
    "        self.qNet.updateQtable(pqv, action)\n",
    "\n",
    "    def setTraining(self, training):\n",
    "        self.Training = training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pet school\n",
    "class PetSchool:\n",
    "    def __init__(self, cat:Cat, numEpisodes, maxEpisodeSteps, training=True, minAlpha = 0.02, eps = 0.2):\n",
    "        self.cat = cat\n",
    "        self.training = training\n",
    "        self.NUM_EPISODES = numEpisodes\n",
    "        self.MAX_EPISODE_STEPS = maxEpisodeSteps\n",
    "        self.alphas = np.linspace(1.0, minAlpha, self.NUM_EPISODES)\n",
    "        self.eps = eps\n",
    "\n",
    "    def train(self):\n",
    "        counter = 0\n",
    "        for e in range(self.NUM_EPISODES): #  episode: a rund for agent\n",
    "            print(\"episode: \", e)\n",
    "            state = self.cat.qNet.gw.initCatState(rd=True) # default is rd = False\n",
    "            self.cat.qNet.setAlpha(self.alphas[e])\n",
    "            total_reward  = 0\n",
    "            step = 0\n",
    "            end = False\n",
    "            for _ in range(self.MAX_EPISODE_STEPS): # step: a time step for agent\n",
    "                action = self.cat.qNet.selectAction(deepcopy(state), self.training)\n",
    "                p, reward, end = self.cat.act(state, action)\n",
    "                self.catMoveTo(p)\n",
    "                # self.cat.updateQtable(action, p, reward, state) # speedup learning\n",
    "                total_reward += reward\n",
    "                step += 1\n",
    "                counter += 1\n",
    "                if end:\n",
    "                    print(\"catch the mouse!!!\")\n",
    "                    print(\"total reward: \", total_reward, \"steps: \", step)\n",
    "                    break\n",
    "        print(\"counter: \", counter)\n",
    "\n",
    "    def catMoveTo(self, p):\n",
    "        self.cat.qNet.gw.setCatP(p)\n",
    "\n",
    "    def show(self):\n",
    "        self.cat.qNet.gw.show()\n",
    "        print(\"qTable: \", self.cat.qNet.qt)\n",
    "        print(\"\\nparams: \", self.cat.qNet.rets)\n",
    "\n",
    "    def initqTable(self, actions, size):\n",
    "        d = {}\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                d[i,j] = np.zeros(len(actions))\n",
    "        return d\n",
    "        \n",
    "    def mouseMove(p,oldPos): # goal (mouse) moves randomly with prob p every time the cat moves\n",
    "        side = 2 # Number of cells per side of the grid\n",
    "        if np.random.random() < p:\n",
    "            n = np.random.random()\n",
    "            if n < 0.25:\n",
    "                newPos = (max(0, oldPos[0]-1),oldPos[1])\n",
    "            elif n < 0.5:\n",
    "                newPos = (min(side - 1, oldPos[0]+1),oldPos[1])\n",
    "            elif n < 0.75:\n",
    "                newPos = (oldPos[0],max(0, oldPos[1]-1))\n",
    "            else:\n",
    "                newPos = (oldPos[0],min(side - 1, oldPos[1]+1))\n",
    "        else:\n",
    "            newPos = oldPos\n",
    "        return newPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super parameter\n",
    "gridSize = [3, 3]\n",
    "catP = [gridSize[0]-1, gridSize[0]-1]\n",
    "mouseP = [0, 0]\n",
    "EPS = 20\n",
    "MAX_EPS_STEP = 30\n",
    "sizeOfParams = 6\n",
    "gamma = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  1\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  2\n",
      "episode:  3\n",
      "episode:  4\n",
      "episode:  5\n",
      "episode:  6\n",
      "episode:  7\n",
      "episode:  8\n",
      "episode:  9\n",
      "episode:  10\n",
      "catch the mouse!!!\n",
      "total reward:  999 steps:  2\n",
      "episode:  11\n",
      "episode:  12\n",
      "catch the mouse!!!\n",
      "total reward:  996 steps:  5\n",
      "episode:  13\n",
      "episode:  14\n",
      "episode:  15\n",
      "episode:  16\n",
      "episode:  17\n",
      "episode:  18\n",
      "episode:  19\n",
      "counter:  489\n",
      "m emp emp \n",
      "emp c emp \n",
      "emp emp emp \n",
      "\n",
      "qTable:  {(0, 0): array([0., 0., 0., 0.]), (0, 1): array([975.90194637, 945.64269226, 996.83885331, 915.58309722]), (0, 2): array([918.486568, 938.2516  ,   0.      , 918.486568]), (1, 0): array([1000.        ,    0.        ,  979.        ,  703.44311136]), (1, 1): array([975.90207546, 938.25158487, 979.        , 938.2516    ]), (1, 2): array([918.48656799, 918.486567  , 958.42      , 938.2516    ]), (2, 0): array([979.    , 958.42  , 958.42  , 938.2516]), (2, 1): array([958.42    , 938.2516  ,   0.      , 918.486568]), (2, 2): array([938.2516    , 918.486568  , 938.25159992, 918.486568  ])}\n",
      "\n",
      "params:  {(0, 0): (array([0.77132064, 0.02075195, 0.63364823, 0.74880388, 0.49850701,\n",
      "       0.22479665]), 0.0, 0), (0, 1): (array([1.2138823 , 3.8219542 , 1.47526168, 0.49127366, 1.88483263,\n",
      "       0.98980923]), 0.00012987253694518586, 79), (0, 2): (array([7.51119833, 7.51893002, 6.09006125, 3.0311237 , 1.28827541,\n",
      "       0.09903994]), -1.0, 41), (1, 0): (array([1.91777412, 0.71457578, 0.54254437, 0.14227005, 0.37334076,\n",
      "       0.67413362]), -1.0, 46), (1, 1): (array([13.44209833,  3.43390573,  2.61792779,  2.51314383,  1.65004708,\n",
      "        0.60069292]), 0.0, 39), (1, 2): (array([7.8053232 , 3.27044325, 2.41073881, 1.65579897, 1.3159226 ,\n",
      "       1.45146555]), 0.0, 43), (2, 0): (array([ 7.0386296 ,  6.30568709,  2.59697521,  0.91350671,  3.63424831,\n",
      "       -0.32924643]), 1.1368683772161603e-13, 40), (2, 1): (array([9.26461646, 6.83477378, 3.00105987, 4.66625066, 1.67430125,\n",
      "       2.03859949]), 0.0, 47), (2, 2): (array([9.3276541 , 2.18255258, 0.51949932, 2.09382618, 1.82304837,\n",
      "       1.15101637]), 0.0, 39)}\n"
     ]
    }
   ],
   "source": [
    "def initqTable(size, actions=[UP, DOWN, LEFT, RIGHT]):\n",
    "    d = {}\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            d[i,j] = np.zeros(len(actions))\n",
    "    return d\n",
    "\n",
    "# initGridWorld\n",
    "gridWorld = GridWorld(gridSize, catP=catP, mouseP=mouseP)\n",
    "# init q Table\n",
    "qt = initqTable(gridSize)\n",
    "# init q Circuit\n",
    "qNet = QNet(qt, gridWorld, initialParameters, gamma=gamma)\n",
    "# init cat\n",
    "cat = Cat(qNet=qNet)\n",
    "# init pet school\n",
    "petSchool = PetSchool(cat, EPS, MAX_EPS_STEP)\n",
    "# start training\n",
    "petSchool.train()\n",
    "# show what have been learned\n",
    "petSchool.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

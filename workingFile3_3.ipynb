{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "from qiskit import Aer, transpile, assemble\n",
    "from qiskit.providers import backend\n",
    "from qiskit.aqua.components.optimizers import COBYLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPES:\n",
    "CAT = \"c\"\n",
    "# DOG = \"d\"\n",
    "MOUSE = \"m\"\n",
    "EMPTY = \"emp\"\n",
    "\n",
    "# ACTIONS:\n",
    "UP = \"00\"\n",
    "DOWN = \"01\"\n",
    "LEFT = \"10\"\n",
    "RIGHT = \"11\"\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "\n",
    "# random seed\n",
    "random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state of cat\n",
    "class State:\n",
    "    def __init__(self, catP):\n",
    "        self.row = catP[0]\n",
    "        self.column = catP[1]\n",
    "        self.catP = catP\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.row == other.row and self.column == other.column and self.catP == other.catP\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.catP))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State(cat_pos={self.catP})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld\n",
    "# e.g.\n",
    "#  MOUSE | EMPTY | EMPTY\n",
    "#  EMPTY | EMPTY | EMPTY\n",
    "#  EMPTY | EMPTY | CAT\n",
    "class GridWorld:\n",
    "    def __init__(self, s, catP, mouseP):\n",
    "        self.numRows = s[0]\n",
    "        self.numColumns = s[1]\n",
    "        self.catP = catP\n",
    "        self.mouseP = mouseP\n",
    "        # self.dogP = dogP\n",
    "        assert(not self.compaireList(self.catP, self.mouseP))\n",
    "    \n",
    "    def getItem(self, p):\n",
    "        if p[0]>=self.numRows or p[0]<0:\n",
    "            return None\n",
    "        if p[1]>=self.numColumns or p[1]<0:\n",
    "            return None\n",
    "        if self.compaireList(p, catP):\n",
    "            return CAT\n",
    "        elif self.compaireList(p, mouseP):\n",
    "            return MOUSE\n",
    "        # elif self.compaireList(p, DOG):\n",
    "        #     return DOG\n",
    "        else:\n",
    "            return EMPTY\n",
    "\n",
    "    def compaireList(self, l1,l2):\n",
    "        for i, j in zip(l1, l2):\n",
    "            if i!=j:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def getNumRows(self):\n",
    "        return self.numRows\n",
    "\n",
    "    def getNumColumns(self):\n",
    "        return self.numColumns\n",
    "\n",
    "    def getMouse(self):\n",
    "        return self.mouse\n",
    "    \n",
    "    def getCatP(self):\n",
    "        return self.catP\n",
    "\n",
    "    def setCatP(self, p):\n",
    "        self.catP = p\n",
    "    \n",
    "    def initCatState(self, rd = False):\n",
    "        # init cat position\n",
    "        if not rd:\n",
    "            catP = [self.getNumRows() - 1, self.getNumColumns() - 1]\n",
    "        else:\n",
    "            catP = [random.randint(0, self.getNumRows()), random.randint(0, self.getNumColumns())]\n",
    "            while self.getItem(catP) != EMPTY and self.getItem(catP) != CAT:\n",
    "                catP = [random.randint(0, self.getNumRows()), random.randint(0, self.getNumColumns())]\n",
    "        self.setCatP(catP)\n",
    "        return State(catP)\n",
    "    \n",
    "    def show(self):\n",
    "        output = \"\"\n",
    "        for i in range(self.numRows):\n",
    "            for j in range(self.numColumns):\n",
    "                if self.compaireList([i,j], self.catP):\n",
    "                    output += CAT + \" \"\n",
    "                if self.compaireList([i,j], self.mouseP):\n",
    "                    output += MOUSE + \" \"\n",
    "                if not self.compaireList([i,j], self.catP) and not self.compaireList([i,j], self.mouseP):\n",
    "                    output += EMPTY + \" \"\n",
    "            output += \"\\n\"\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNet\n",
    "class QNet:\n",
    "    \n",
    "    def __init__(self, qTable, gridWorld:GridWorld, params, alpha=0.1, gamma=1.0, eps=0.2, actions=[UP, DOWN, LEFT, RIGHT]):\n",
    "        \n",
    "        self.params = params # inital parameters are the same for all qNetwork\n",
    "        self.gw = gridWorld\n",
    "        self.qt = qTable\n",
    "        self.eps = eps\n",
    "        self.backend = Aer.get_backend(\"qasm_simulator\")\n",
    "        self.NUM_SHOTS = 1000 # number of measurements \n",
    "        self.optimizer = COBYLA(maxiter=500, tol=0.0001) # off the shelf\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.ACTIONS = actions\n",
    "\n",
    "        # self.rets = {(0,0):([0,..,0],0.0,0), ...}\n",
    "        self.rets = dict() # resulting parameters after optimization for all points in the grid\n",
    "\n",
    "        self.state = None\n",
    "        \n",
    "        for i in range(self.gw.getNumRows()):\n",
    "            for j in range(self.gw.getNumColumns()):\n",
    "                self.rets[i, j] = (self.params, 0.0, 0) \n",
    "    \n",
    "    def qcMaker(self, params):\n",
    "        qr = QuantumRegister(2, name=\"q\")\n",
    "        cr = ClassicalRegister(2, name=\"c\")\n",
    "        qc = QuantumCircuit(qr, cr)\n",
    "        qc.u3(params[0], params[1], params[2], qr[0])\n",
    "        qc.u3(params[3], params[4], params[5], qr[1])\n",
    "        # qc.cx(qr[0], qr[1])\n",
    "        qc.measure(qr, cr)\n",
    "        return qc\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "        p = deepcopy(state.catP)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(self.gw.getNumRows() - 1, p[0]+1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(self.gw.getNumColumns() - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unkown action {action}\")\n",
    "        return p\n",
    "        \n",
    "    def getReward(self, p):\n",
    "        grid = self.gw.getItem(p)\n",
    "        if grid == EMPTY:\n",
    "            reward = -1\n",
    "        # elif grid == DOG:\n",
    "        #     reward = -1000\n",
    "        elif grid == MOUSE:\n",
    "            reward = 100\n",
    "        elif grid == CAT:\n",
    "            reward = -1 # (maybe less than reward of empty)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward\n",
    "    \n",
    "    def selectAction(self, state, training):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            return random.choice(self.ACTIONS)\n",
    "        else:\n",
    "            if training:\n",
    "                self.state = state\n",
    "                self.updateCircuit(state)\n",
    "            return self.ACTIONS[np.argmax(self.qt[self.state.catP[0], self.state.catP[1]])]\n",
    "        \n",
    "    def lossFunction(self, params):\n",
    "        qc = self.qcMaker(params=params)\n",
    "        t_qc = transpile(qc, self.backend)\n",
    "        job = assemble(t_qc, shots=self.NUM_SHOTS)\n",
    "        rlt = self.backend.run(job).result()\n",
    "        counts = rlt.get_counts(qc) \n",
    "        action = max(counts, key = counts.get)\n",
    "        nextPosition = self.newPosition(self.state, action) # handle the \n",
    "        reward = self.getReward(nextPosition)\n",
    "        # update q-table(but not very sure, update only for this action or for all actions)\n",
    "        targetQvalue = reward + self.gamma *  np.max(self.qt[nextPosition[0],nextPosition[1]])\n",
    "        predictedQvalue = self.calculateQvalue(action, nextPosition, reward, self.state)\n",
    "        \n",
    "        # update q-table\n",
    "        self.updateQtable(predictedQvalue, action)\n",
    "        return targetQvalue - self.qt[self.state.catP[0],self.state.catP[1]][int(action,2)]\n",
    "    \n",
    "    def updateQtable(self, predictedQvalue, action):\n",
    "        if self.qt[(self.state.catP[0],self.state.catP[1])][int(action,2)] < predictedQvalue:\n",
    "            self.qt[self.state.catP[0],self.state.catP[1]][int(action,2)] = predictedQvalue\n",
    "\n",
    "    def calculateQvalue(self, action, nextPosition, reward, state:State):\n",
    "        targetQvalue = reward + self.gamma *  np.max(self.qt[nextPosition[0],nextPosition[1]])\n",
    "        return self.qt[state.catP[0], state.catP[1]][int(action,2)] + self.alpha * (targetQvalue - self.qt[state.catP[0],state.catP[1]][int(action,2)]) # update q-table\n",
    "\n",
    "    def updateCircuit(self, state:State):\n",
    "        self.rets[state.catP[0], state.catP[1]] = self.optimizer.optimize(num_vars=6, objective_function=self.lossFunction, initial_point=self.rets[state.catP[0], state.catP[1]][0])\n",
    "\n",
    "    def setAlpha(self, alpha):\n",
    "        self.alpha = alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent: cat\n",
    "class Cat:\n",
    "    def __init__(self, qNet: QNet, training=True, eps = 0.2, actions = [UP, DOWN, LEFT, RIGHT]):\n",
    "        self.eps = eps\n",
    "        self.training = training\n",
    "        self.qNet = qNet\n",
    "        self.ACTIONS = actions\n",
    "        self.state = None\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "            p = deepcopy(state.catP)\n",
    "            if action == UP:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "            elif action == DOWN:\n",
    "                p[0] = min(self.qNet.gw.getNumRows() - 1, p[0] + 1)\n",
    "            elif action == LEFT:\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == RIGHT:\n",
    "                p[1] = min(self.qNet.gw.getNumColumns() - 1, p[1] + 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unkown action {self.ACTIONS[action]}\")\n",
    "            return p\n",
    "\n",
    "    def getReward(self, p):\n",
    "        grid = self.qNet.gw.getItem(p)\n",
    "        if grid == MOUSE:\n",
    "            reward = 1000\n",
    "            end = True\n",
    "            self.qNet.gw.setCatP(p)\n",
    "        # elif grid == DOG:\n",
    "        #     reward = -100\n",
    "        #     end = True\n",
    "        #     self.qNet.gw.setCatP(p)\n",
    "        elif grid == EMPTY:\n",
    "            reward = -1\n",
    "            end = False\n",
    "            self.qNet.gw.setCatP(p)\n",
    "        elif grid == CAT:\n",
    "            reward = -1 # (maybe less than reward of empty)\n",
    "            end = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward, end\n",
    "\n",
    "    def act(self, state, action):\n",
    "        p = self.newPosition(state, action)\n",
    "        reward, end = self.getReward(p)\n",
    "        return p, reward, end\n",
    "    \n",
    "    def updateQtable(self, action, p, reward, state):\n",
    "        pqv = self.qNet.calculateQvalue(action, p, reward, state)\n",
    "        self.qNet.updateQtable(pqv, action)\n",
    "\n",
    "    def setTraining(self, training):\n",
    "        self.Training = training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pet school\n",
    "class PetSchool:\n",
    "    def __init__(self, cat:Cat, numEpisodes, maxEpisodeSteps, training=True, minAlpha = 0.02, eps = 0.2):\n",
    "        self.cat = cat\n",
    "        self.training = training\n",
    "        self.NUM_EPISODES = numEpisodes\n",
    "        self.MAX_EPISODE_STEPS = maxEpisodeSteps\n",
    "        self.alphas = np.linspace(1.0, minAlpha, self.NUM_EPISODES)\n",
    "        self.eps = eps\n",
    "\n",
    "    def train(self):\n",
    "        counter = 0\n",
    "        for e in range(self.NUM_EPISODES): #  episode: a rund for agent\n",
    "            print(\"episode: \", e)\n",
    "            state = self.cat.qNet.gw.initCatState(rd=True) # default is rd = False\n",
    "            self.cat.qNet.setAlpha(self.alphas[e])\n",
    "            total_reward  = 0\n",
    "            step = 0\n",
    "            end = False\n",
    "            for _ in range(self.MAX_EPISODE_STEPS): # step: a time step for agent\n",
    "                action = self.cat.qNet.selectAction(deepcopy(state), self.training)\n",
    "                p, reward, end = self.cat.act(state, action)\n",
    "                self.cat.updateQtable(action, p, reward, state)\n",
    "                total_reward += reward\n",
    "                step += 1\n",
    "                counter += 1\n",
    "                if end:\n",
    "                    print(\"catch the mouse!!!\")\n",
    "                    print(\"total reward: \", total_reward, \"steps: \", step)\n",
    "                    break\n",
    "        print(\"counter: \", counter)\n",
    "\n",
    "    def show(self):\n",
    "        self.cat.qNet.gw.show()\n",
    "        print(\"qTable: \", self.cat.qNet.qt)\n",
    "        print(\"\\nparams: \", self.cat.qNet.rets)\n",
    "\n",
    "    def initqTable(self, actions, size):\n",
    "        d = {}\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                d[i,j] = np.zeros(len(actions))\n",
    "        return d\n",
    "        \n",
    "    def mouseMove(p,oldPos): # goal (mouse) moves randomly with prob p every time the cat moves\n",
    "        side = 2 # Number of cells per side of the grid\n",
    "        if np.random.random() < p:\n",
    "            n = np.random.random()\n",
    "            if n < 0.25:\n",
    "                newPos = (max(0, oldPos[0]-1),oldPos[1])\n",
    "            elif n < 0.5:\n",
    "                newPos = (min(side - 1, oldPos[0]+1),oldPos[1])\n",
    "            elif n < 0.75:\n",
    "                newPos = (oldPos[0],max(0, oldPos[1]-1))\n",
    "            else:\n",
    "                newPos = (oldPos[0],min(side - 1, oldPos[1]+1))\n",
    "        else:\n",
    "            newPos = oldPos\n",
    "        return newPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super parameter\n",
    "gridSize = [3, 3]\n",
    "catP = [gridSize[0]-1, gridSize[0]-1]\n",
    "mouseP = [0, 0]\n",
    "EPS = 20\n",
    "MAX_EPS_STEP = 30\n",
    "sizeOfParams = 6\n",
    "gamma = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  1\n",
      "episode:  2\n",
      "episode:  3\n",
      "catch the mouse!!!\n",
      "total reward:  988 steps:  13\n",
      "episode:  4\n",
      "catch the mouse!!!\n",
      "total reward:  999 steps:  2\n",
      "episode:  5\n",
      "episode:  6\n",
      "episode:  7\n",
      "episode:  8\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  9\n",
      "episode:  10\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  11\n",
      "episode:  12\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  13\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  14\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  15\n",
      "episode:  16\n",
      "episode:  17\n",
      "catch the mouse!!!\n",
      "total reward:  999 steps:  2\n",
      "episode:  18\n",
      "episode:  19\n",
      "counter:  353\n",
      "m c emp \n",
      "emp emp emp \n",
      "emp emp emp \n",
      "\n",
      "qTable:  {(0, 0): array([0., 0., 0., 0.]), (0, 1): array([827.34472974, 930.63529982, 988.44925348,   0.        ]), (0, 2): array([818.21466349, 776.44810475, 841.91360752,  36.41080438]), (1, 0): array([1000.        ,    0.        ,    0.        ,  221.56315789]), (1, 1): array([953.90737143, 927.11710565, 979.        , 129.51094883]), (1, 2): array([  0.        ,   0.        , 958.42      , 844.39511916]), (2, 0): array([979.        , 958.40285591, 958.42      , 861.27662776]), (2, 1): array([954.04080507, 880.31550065, 958.42      , 978.65623755]), (2, 2): array([1000.        ,  863.705811  ,    0.        ,  823.30925296])}\n",
      "\n",
      "params:  {(0, 0): (array([0., 0., 0., 0., 0., 0.]), 0.0, 0), (0, 1): (array([2.97848546, 2.201877  , 2.04165041, 2.00859314, 1.0017729 ,\n",
      "       1.00036578]), -1.0, 37), (0, 2): (array([ 1.50714332, 25.75755285, 22.93918008,  1.41264503,  0.78758433,\n",
      "        4.65827625]), 3.3453386547114405, 56), (1, 0): (array([0., 0., 0., 0., 0., 0.]), -900.0, 37), (1, 1): (array([0.06010161, 2.74136339, 2.09321085, 1.89537013, 1.61620285,\n",
      "       1.67853099]), 0.0, 37), (1, 2): (array([0., 0., 0., 0., 0., 0.]), -1.0, 37), (2, 0): (array([-0.02334151,  1.45054923,  1.48771125,  1.27548869,  1.48778508,\n",
      "        1.27549177]), 0.0, 37), (2, 1): (array([1.19124668, 3.08824892, 1.841961  , 1.65141193, 1.50398713,\n",
      "       1.38992688]), 0.0, 38), (2, 2): (array([-1.12732416,  0.35149853,  1.18750826, -0.23885926,  0.48214834,\n",
      "        2.38108411]), 1.1368683772161603e-13, 37)}\n"
     ]
    }
   ],
   "source": [
    "def initqTable(size, actions=[UP, DOWN, LEFT, RIGHT]):\n",
    "    d = {}\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            d[i,j] = np.zeros(len(actions))\n",
    "    return d\n",
    "\n",
    "# initGridWorld\n",
    "gridWorld = GridWorld(gridSize, catP=catP, mouseP=mouseP)\n",
    "# init parameter\n",
    "initialParameters = np.zeros(sizeOfParams)\n",
    "# init q Table\n",
    "qt = initqTable(gridSize)\n",
    "# init q Circuit\n",
    "qNet = QNet(qt, gridWorld, initialParameters, gamma=gamma)\n",
    "# init cat\n",
    "cat = Cat(qNet=qNet)\n",
    "# init pet school\n",
    "petSchool = PetSchool(cat, EPS, MAX_EPS_STEP)\n",
    "# start training\n",
    "petSchool.train()\n",
    "# show what have been learned\n",
    "petSchool.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

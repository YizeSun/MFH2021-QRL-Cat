{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "from qiskit import Aer, transpile, assemble\n",
    "from qiskit.providers import backend\n",
    "from qiskit.aqua.components.optimizers import COBYLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "CAT = \"c\"\n",
    "DOG = \"d\"\n",
    "MOUSE = \"m\"\n",
    "EMPTY = \"emp\"\n",
    "\n",
    "# gridWorld = [[MOUSE, EMPTY, DOG],\n",
    "#         [EMPTY, EMPTY, EMPTY],\n",
    "#          [DOG, EMPTY, CAT]]\n",
    "\n",
    "# actions:\n",
    "UP = \"00\"\n",
    "DOWN = \"01\"\n",
    "LEFT = \"10\"\n",
    "RIGHT = \"11\"\n",
    "\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m emp d\n",
      "emp emp emp\n",
      "d emp c\n"
     ]
    }
   ],
   "source": [
    "def showGridWorld():\n",
    "    for row in gridWorld:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "showGridWorld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, catP):\n",
    "        self.row = catP[0]\n",
    "        self.column = catP[1]\n",
    "        self.catP = catP\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.row == other.row and self.column == other.column and self.catP == other.catP\n",
    "\n",
    "    def __hash__(self): # for making dictionary\n",
    "        return hash(str(self.catP))\n",
    "# q-table={(0,0):[1,2,3,4]}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"State(cat_pos={self.catP})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old version without any changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent: cat\n",
    "class cat:\n",
    "    def __init__(self, eps, qTable, gridWorld, training):\n",
    "        self.eps = eps\n",
    "        self.qt = qTable #dictionary\n",
    "        self.gw = gridWorld # field \n",
    "        self.params = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "        self.backend = Aer.get_backend(\"qasm_simulator\")\n",
    "        self.NUM_SHOTS = 1000 #number of measurements \n",
    "        self.optimizer = COBYLA(maxiter=500, tol=0.0001) # off the shelf\n",
    "        self.training = training # For future development \n",
    "        self.rets = None # resulting parameters after optimization for all points in the grid\n",
    "        self.qcs = self.initQC(ret[0]) # QUESTION!!! Random or not???\n",
    "        self.qc = None\n",
    "        self.state = None\n",
    "    \n",
    "    def initQC(self, params):\n",
    "        qcs = {}\n",
    "        def qcMaker(params):\n",
    "            qr = QuantumRegister(2, name=\"q\")\n",
    "            cr = ClassicalRegister(2, name=\"c\")\n",
    "            qc = QuantumCircuit(qr, cr)\n",
    "            qc.u3(params[0], params[1], params[2], qr[0])\n",
    "            qc.u3(params[3], params[4], params[5], qr[1])\n",
    "            qc.cx(qr[0], qr[1])\n",
    "            qc.measure(qr, cr)\n",
    "            return qc\n",
    "\n",
    "        for i in range(len(self.gw)):\n",
    "            for j in range(len(self.gw[i])):\n",
    "                qc = qcMaker(params)\n",
    "                qcs[i, j] = qc\n",
    "        return qcs\n",
    "\n",
    "    def selectAction(self, state):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            return random.choice(ACTIONS)\n",
    "        else:\n",
    "            if self.training:\n",
    "                self.qc = self.qcs[state.row, state.column]\n",
    "                self.state = state\n",
    "                self.updateCircuit(state)\n",
    "            return np.argmax(self.qt[state])\n",
    "\n",
    "    def lossFunction(self, params):\n",
    "        state = self.state\n",
    "        qc = self.qc\n",
    "        t_qc = transpile(qc, self.backend)\n",
    "        job = assemble(t_qc, shots=self.NUM_SHOTS)\n",
    "        rlt = self.backend.run(job).result()\n",
    "        counts = rlt.get_counts(qc)\n",
    "        action = max(counts, key = counts.get)\n",
    "        nextPosition = self.newPosition(state, action) # handle the \n",
    "        reward, _ = self.getReward(nextPosition)\n",
    "        # update q-table(but not very sure, update only for this action or for all actions)\n",
    "        # I think only for this action - vasya\n",
    "        targetQvalue = reward + gamma *  np.max(self.qt[State(nextPosition)])\n",
    "        if targetQvalue - self.qt[state][action] > 0:\n",
    "            self.qt[state][action] += alpha * (targetQvalue - self.qt[state][action]) # update q-table\n",
    "        return targetQvalue - self.qt[state][action]\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "            p = deepcopy(state.catP)\n",
    "            if action == UP:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "            elif action == DOWN:\n",
    "                p[0] = min(len(self.gw) - 1, p[0]+1)\n",
    "            elif action == LEFT:\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == RIGHT:\n",
    "                p[1] = min(len(self.gw) - 1, p[1] + 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unkown action {action}\")\n",
    "            return p\n",
    "\n",
    "    def getReward(self, p):\n",
    "        grid = self.gw[p[0]][p[1]]\n",
    "        if grid == DOG:\n",
    "            reward = -100\n",
    "            end = True\n",
    "            self.gw[p[0]][p[1]] += CAT\n",
    "        elif grid == MOUSE:\n",
    "            reward = 100\n",
    "            end = True\n",
    "            self.gw[p[0]][p[1]] += CAT\n",
    "        elif grid == EMPTY:\n",
    "            reward = -1\n",
    "            end = False\n",
    "            old = state.catP\n",
    "            self.gw[old[0]][old[1]] = EMPTY\n",
    "            self.gw[p[0]][p[1]] = CAT\n",
    "        elif grid == CAT:\n",
    "            reward = -1\n",
    "            end = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward, end\n",
    "\n",
    "    def act(self, state, action):\n",
    "        p = self.newPosition(state, action)\n",
    "        reward, end = self.getReward(p)\n",
    "        return deepcopy[p], reward, end\n",
    "        \n",
    "    def updateCircuit(self, state):\n",
    "        self.rets[state] = self.optimizer.optimize(num_vars=6, objective_function=self.lossFunction, initial_point=self.params)\n",
    "    \n",
    "    def setTraining(self, training):\n",
    "        self.Training = training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Version with qNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent: cat\n",
    "class cat:\n",
    "    \n",
    "    def __init__(self, eps, qTable, gridWorld, training):\n",
    "        self.eps = eps\n",
    "        self.qt = qTable #dictionary\n",
    "        self.gw = gridWorld # field \n",
    "        self.params = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "        self.training = training # For future development \n",
    "        self.qcs = self.initQC(ret[0]) # QUESTION!!! Random or not???\n",
    "        \n",
    "        self.state = None\n",
    "        \n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "            p = deepcopy(state.catP)\n",
    "            if action == UP:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "            elif action == DOWN:\n",
    "                p[0] = min(len(self.gw) - 1, p[0]+1)\n",
    "            elif action == LEFT:\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == RIGHT:\n",
    "                p[1] = min(len(self.gw) - 1, p[1] + 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unkown action {action}\")\n",
    "            return p\n",
    "\n",
    "    def getReward(self, p):\n",
    "        grid = self.gw[p[0]][p[1]]\n",
    "        if grid == DOG:\n",
    "            reward = -100\n",
    "            end = True\n",
    "            self.gw[p[0]][p[1]] += CAT\n",
    "        elif grid == MOUSE:\n",
    "            reward = 100\n",
    "            end = True\n",
    "            self.gw[p[0]][p[1]] += CAT\n",
    "        elif grid == EMPTY:\n",
    "            reward = -1\n",
    "            end = False\n",
    "            old = state.catP\n",
    "            self.gw[old[0]][old[1]] = EMPTY\n",
    "            self.gw[p[0]][p[1]] = CAT\n",
    "        elif grid == CAT:\n",
    "            reward = -1\n",
    "            end = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward, end\n",
    "\n",
    "    def act(self, state, action):\n",
    "        p = self.newPosition(state, action)\n",
    "        reward, end = self.getReward(p)\n",
    "        return deepcopy[p], reward, end\n",
    "        \n",
    "    def setTraining(self, training):\n",
    "        self.training = training # The big T was there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantum circuit: state->action\n",
    "class qNetwork:# just a place for \n",
    "    \n",
    "    def __init__(self, qTable, gridWorld, params):\n",
    "        \n",
    "        self.params = params # inital parameters are the same for all qNetwork\n",
    "        self.gw = gridWorld\n",
    "        self.qt = qTable\n",
    "        self.backend = Aer.get_backend(\"qasm_simulator\")\n",
    "        self.NUM_SHOTS = 1000 # number of measurements \n",
    "        self.optimizer = COBYLA(maxiter=500, tol=0.0001) # off the shelf\n",
    "        \n",
    "        self.qcs = None # all qubits\n",
    "        self.rets = None # resulting parameters after optimization for all points in the grid\n",
    "        \n",
    "        \n",
    "        self.qc = None #current state\n",
    "        self.state = None\n",
    "        \n",
    "    \n",
    "        qcs = {}\n",
    "        def qcMaker(params):\n",
    "            qr = QuantumRegister(2, name=\"q\")\n",
    "            cr = ClassicalRegister(2, name=\"c\")\n",
    "            qc = QuantumCircuit(qr, cr)\n",
    "            qc.u3(params[0], params[1], params[2], qr[0])\n",
    "            qc.u3(params[3], params[4], params[5], qr[1])\n",
    "            qc.cx(qr[0], qr[1])\n",
    "            qc.measure(qr, cr)\n",
    "            return qc\n",
    "\n",
    "        for i in range(len(self.gw)):\n",
    "            for j in range(len(self.gw[i])):\n",
    "                qc = qcMaker(params)\n",
    "                qcs[i, j] = qc \n",
    "    \n",
    "        self.qcs = self.initQC(params)\n",
    "    \n",
    "    \n",
    "    def selectAction(self, state, training):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            return random.choice(ACTIONS)\n",
    "        else:\n",
    "            if training:\n",
    "                qc = self.qcs[state.row, state.column]\n",
    "                self.state = state\n",
    "                self.updateCircuit(state)\n",
    "            return np.argmax(self.qt[state])\n",
    "        \n",
    "    def lossFunction(self, params):\n",
    "        state = self.state\n",
    "        qc = self.qc\n",
    "        t_qc = transpile(qc, self.backend)\n",
    "        job = assemble(t_qc, shots=self.NUM_SHOTS)\n",
    "        rlt = self.backend.run(job).result()\n",
    "        counts = rlt.get_counts(qc)\n",
    "        action = max(counts, key = counts.get)\n",
    "        nextPosition = self.newPosition(state, action) # handle the \n",
    "        reward, _ = self.getReward(nextPosition)\n",
    "        # update q-table(but not very sure, update only for this action or for all actions)\n",
    "        targetQvalue = reward + gamma *  np.max(self.qt[State(nextPosition)])\n",
    "        if targetQvalue - self.qt[state][action] > 0:\n",
    "            self.qt[state][action] += alpha * (targetQvalue - self.qt[state][action]) # update q-table\n",
    "        return targetQvalue - self.qt[state][action]\n",
    "\n",
    "    \n",
    "    def updateCircuit(self, state):\n",
    "        self.rets[state] = self.optimizer.optimize(num_vars=6, objective_function=self.lossFunction, initial_point=self.params)\n",
    "\n",
    "        \n",
    "        \n",
    "# TODO: \n",
    "# experiment: petSchool\n",
    "class petSchool:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super parameters\n",
    "N_STATES = 4\n",
    "N_EPISODES = 20\n",
    "\n",
    "MAX_EPISODE_STEPS = 100\n",
    "\n",
    "MIN_ALPHA = 0.02\n",
    "\n",
    "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "initState = State([1,1])\n",
    "initalParameters = [] # Same for all qubits! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qNetwork = qNetwork(qTable, gridWorld, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initqTable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-56b921f8aeac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#  episode: a rund for agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mqTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitqTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtotal_reward\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initqTable' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for e in range(N_EPISODES): #  episode: a rund for agent\n",
    "    \n",
    "    state = initState \n",
    "    qTable = initqTable\n",
    "    total_reward  = 0\n",
    "    alpha = alphas[e]\n",
    "    counter = 0\n",
    "    step = 0\n",
    "    \n",
    "    \n",
    "    while(step < MAX_EPISODE_STEPS): # step: a time step for agent\n",
    "        action = selectAction(state)\n",
    "        newPosition, reward, end = act(state, action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        #update Network\n",
    "        qNetwork.updateCircuit(newPosition)\n",
    "\n",
    "agent.setTraining(False)\n",
    "showResult(agent.qt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

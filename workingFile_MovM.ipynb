{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
    "from qiskit import Aer, transpile, assemble\n",
    "from qiskit.providers import backend\n",
    "from qiskit.aqua.components.optimizers import COBYLA\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPES:\n",
    "CAT = \"c\"\n",
    "# DOG = \"d\"\n",
    "MOUSE = \"m\"\n",
    "EMPTY = \"emp\"\n",
    "\n",
    "# ACTIONS:\n",
    "UP = \"000\"\n",
    "DOWN = \"001\"\n",
    "LEFT = \"010\"\n",
    "RIGHT = \"011\"\n",
    "\n",
    "UPPERLEFT = \"100\"\n",
    "UPPERRIGHT = \"101\"\n",
    "LOWERLEFT = \"110\"\n",
    "LOWERRIGHT = \"111\"\n",
    "\n",
    "# ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT, UPPERLEFT, UPPERRIGHT, LOWERLEFT, LOWERRIGHT]\n",
    "\n",
    "# random seed\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state of cat\n",
    "class State:\n",
    "    def __init__(self, catP, mouseP):\n",
    "        self.catP = catP\n",
    "        self.mouseP = mouseP\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.mouseP==other.mouseP and self.catP == other.catP\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.catP))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State(cat_pos={self.catP})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridWorld\n",
    "# e.g.\n",
    "#  MOUSE | EMPTY | EMPTY\n",
    "#  EMPTY | EMPTY | EMPTY\n",
    "#  EMPTY | EMPTY | CAT\n",
    "class GridWorld:\n",
    "    def __init__(self, s, catP, mouseP):\n",
    "        self.numRows = s[0]\n",
    "        self.numColumns = s[1]\n",
    "        self.catP = catP\n",
    "        self.mouseP = mouseP\n",
    "        # self.dogP = dogP\n",
    "        assert(not self.compaireList(self.catP, self.mouseP))\n",
    "    \n",
    "    def getItem(self, p):\n",
    "        if p[0]>=self.numRows or p[0]<0:\n",
    "            return None\n",
    "        if p[1]>=self.numColumns or p[1]<0:\n",
    "            return None\n",
    "        if self.compaireList(p, catP):\n",
    "            return CAT\n",
    "        elif self.compaireList(p, mouseP):\n",
    "            return MOUSE\n",
    "        # elif self.compaireList(p, DOG):\n",
    "        #     return DOG\n",
    "        else:\n",
    "            return EMPTY\n",
    "\n",
    "    def compaireList(self, l1,l2):\n",
    "        for i, j in zip(l1, l2):\n",
    "            if i!=j:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def getNumRows(self):\n",
    "        return self.numRows\n",
    "\n",
    "    def getNumColumns(self):\n",
    "        return self.numColumns\n",
    "\n",
    "    def getMouseP(self):\n",
    "        return self.mouseP\n",
    "    \n",
    "    def getCatP(self):\n",
    "        return self.catP\n",
    "\n",
    "    def setCatP(self, p):\n",
    "        self.catP = p\n",
    "        \n",
    "    def setMouseP(self, p):\n",
    "        self.mouseP = p\n",
    "    \n",
    "    def initCatState(self, rd = False):\n",
    "        # init cat position\n",
    "        if not rd:\n",
    "            catP = [self.getNumRows() - 1, self.getNumColumns() - 1]\n",
    "        else:\n",
    "            catP = [random.randint(0, self.getNumRows()), random.randint(0, self.getNumColumns())]\n",
    "            while self.getItem(catP) != EMPTY and self.getItem(catP) != CAT:\n",
    "                catP = [random.randint(0, self.getNumRows()), random.randint(0, self.getNumColumns())]\n",
    "        self.setCatP(catP)\n",
    "        return State(catP, self.getMouseP())\n",
    "    \n",
    "    def show(self):\n",
    "        output = \"\"\n",
    "        for i in range(self.numRows):\n",
    "            for j in range(self.numColumns):\n",
    "                if self.compaireList([i,j], self.catP):\n",
    "                    output += CAT + \" \"\n",
    "                if self.compaireList([i,j], self.mouseP):\n",
    "                    output += MOUSE + \" \"\n",
    "                if not self.compaireList([i,j], self.catP) and not self.compaireList([i,j], self.mouseP):\n",
    "                    output += EMPTY + \" \"\n",
    "            output += \"\\n\"\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QNet\n",
    "class QNet:\n",
    "    \n",
    "    def __init__(self, qTable, gridWorld:GridWorld, alpha=0.1, gamma=1.0, eps=0.2, actions=[UP, DOWN, LEFT, RIGHT], numParams=9):\n",
    "        self.gw = gridWorld\n",
    "        self.qt = qTable\n",
    "        self.eps = eps\n",
    "        self.backend = Aer.get_backend(\"qasm_simulator\")\n",
    "        self.NUM_SHOTS = 1000 # number of measurements \n",
    "        self.optimizer = COBYLA(maxiter=500, tol=0.0001) # off the shelf\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.ACTIONS = actions\n",
    "\n",
    "        # self.rets = {(0,0):([0,..,0],0.0,0), ...}\n",
    "        self.rets = dict() # resulting parameters after optimization for all points in the grid\n",
    "\n",
    "        self.state = None\n",
    "        \n",
    "        for i in range(self.gw.getNumRows()):\n",
    "            for j in range(self.gw.getNumColumns()):\n",
    "                for m in range(self.gw.getNumRows()):\n",
    "                    for n in range(self.gw.getNumColumns()):\n",
    "                        self.rets[i, j, m, n] = (np.random.rand(numParams), 0.0, 0) \n",
    "    \n",
    "    def qcMaker(self, params):\n",
    "        qr = QuantumRegister(3, name=\"q\")\n",
    "        cr = ClassicalRegister(3, name=\"c\")\n",
    "        qc = QuantumCircuit(qr, cr)\n",
    "        start = 0\n",
    "        step = 3\n",
    "        assert(not len(params)%3)\n",
    "        # add U3 for all qubits once\n",
    "        for i in range(int(len(params)/3)): \n",
    "            qc.u3(params[start], params[start + 1], params[start + 2], qr[i])\n",
    "            start += step\n",
    "        # qc.cx(qr[0], qr[1])\n",
    "        qc.measure(qr, cr)\n",
    "        return qc\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "        p = deepcopy(state.catP)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(self.gw.getNumRows() - 1, p[0]+1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(self.gw.getNumColumns() - 1, p[1] + 1)\n",
    "        elif action == UPPERLEFT:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == UPPERRIGHT:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "            p[1] = min(self.gw.getNumColumns() - 1, p[1] + 1)\n",
    "        elif action == LOWERLEFT:\n",
    "            p[0] = min(self.gw.getNumRows() - 1, p[0] + 1)\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == LOWERRIGHT:\n",
    "            p[0] = min(self.gw.getNumRows() - 1, p[0] + 1)\n",
    "            p[1] = min(self.gw.getNumColumns() - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unkown action {action}\")\n",
    "        return p\n",
    "        \n",
    "    def getReward(self, p):\n",
    "        grid = self.gw.getItem(p)\n",
    "        if grid == EMPTY:\n",
    "            reward = -1\n",
    "        # elif grid == DOG:\n",
    "        #     reward = -1000\n",
    "        elif grid == MOUSE:\n",
    "            reward = 1000\n",
    "        elif grid == CAT:\n",
    "            reward = -1 # (maybe less than reward of empty)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward\n",
    "    \n",
    "    def selectAction(self, state, training):\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            return random.choice(self.ACTIONS)\n",
    "        else:\n",
    "            if training:\n",
    "                self.state = state\n",
    "                self.updateCircuit(state)\n",
    "            return self.ACTIONS[np.argmax(self.qt[state.catP[0], state.catP[1], state.mouseP[0], state.mouseP[1]])]\n",
    "        \n",
    "    def lossFunction(self, params):\n",
    "        action = \"\"\n",
    "        qc = self.qcMaker(params=params)\n",
    "        t_qc = transpile(qc, self.backend)\n",
    "        job = assemble(t_qc, shots=self.NUM_SHOTS)\n",
    "        rlt = self.backend.run(job).result()\n",
    "        counts = rlt.get_counts(qc)\n",
    "        # speedup training, cross the ravine\n",
    "        if random.uniform(0, 1) < self.eps:\n",
    "            action = random.choice(self.ACTIONS)\n",
    "        else:\n",
    "            action = max(counts, key = counts.get)\n",
    "        nextPosition = self.newPosition(self.state, action) # handle the \n",
    "        reward = self.getReward(nextPosition)\n",
    "        # update q-table(but not very sure, update only for this action or for all actions)\n",
    "        targetQvalue = reward + self.gamma *  np.max(self.qt[nextPosition[0],nextPosition[1], self.state.mouseP[0], self.state.mouseP[1]])\n",
    "        predictedQvalue = self.calculateQvalue(action, nextPosition, reward, targetQvalue, self.state)\n",
    "        \n",
    "        # update q-table\n",
    "        self.updateQtable(predictedQvalue, action)\n",
    "        return targetQvalue - self.qt[self.state.catP[0], self.state.catP[1], self.state.mouseP[0], self.state.mouseP[1]][int(action,2)]\n",
    "    \n",
    "    def updateQtable(self, predictedQvalue, action):\n",
    "        if self.qt[self.state.catP[0], self.state.catP[1], self.state.mouseP[0], self.state.mouseP[1]][int(action,2)] < predictedQvalue:\n",
    "            self.qt[self.state.catP[0], self.state.catP[1], self.state.mouseP[0], self.state.mouseP[1]][int(action,2)] = predictedQvalue\n",
    "\n",
    "    def calculateQvalue(self, action, nextPosition, reward, targetQvalue, state:State):\n",
    "        return self.qt[state.catP[0], state.catP[1], state.mouseP[0], state.mouseP[1]][int(action,2)] + self.alpha * (targetQvalue - self.qt[state.catP[0],state.catP[1],state.mouseP[0], state.mouseP[1]][int(action,2)]) # update q-table\n",
    "\n",
    "    def updateCircuit(self, state:State):\n",
    "        self.rets[state.catP[0], state.catP[1], state.mouseP[0], state.mouseP[1]] = self.optimizer.optimize(num_vars=9, objective_function=self.lossFunction, initial_point=self.rets[state.catP[0], state.catP[1], state.mouseP[0], state.mouseP[1]][0])\n",
    "\n",
    "    def setAlpha(self, alpha):\n",
    "        self.alpha = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent: cat\n",
    "class Cat:\n",
    "    def __init__(self, qNet: QNet, training=True, eps = 0.2, actions = [UP, DOWN, LEFT, RIGHT]):\n",
    "        self.eps = eps\n",
    "        self.training = training\n",
    "        self.qNet = qNet\n",
    "        self.ACTIONS = actions\n",
    "        self.state = None\n",
    "\n",
    "    def newPosition(self, state, action):\n",
    "            p = deepcopy(state.catP)\n",
    "            if action == UP:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "            elif action == DOWN:\n",
    "                p[0] = min(self.qNet.gw.getNumRows() - 1, p[0] + 1)\n",
    "            elif action == LEFT:\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == RIGHT:\n",
    "                p[1] = min(self.qNet.gw.getNumColumns() - 1, p[1] + 1)\n",
    "            # in case of tough boundary then slipe\n",
    "            elif action == UPPERLEFT:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == UPPERRIGHT:\n",
    "                p[0] = max(0, p[0] - 1)\n",
    "                p[1] = min(self.qNet.gw.getNumColumns() - 1, p[1] + 1)\n",
    "            elif action == LOWERLEFT:\n",
    "                p[0] = min(self.qNet.gw.getNumRows() - 1, p[0] + 1)\n",
    "                p[1] = max(0, p[1] - 1)\n",
    "            elif action == LOWERRIGHT:\n",
    "                p[0] = min(self.qNet.gw.getNumRows() - 1, p[0] + 1)\n",
    "                p[1] = min(self.qNet.gw.getNumColumns() - 1, p[1] + 1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unkown action {self.ACTIONS[action]}\")\n",
    "            return p\n",
    "\n",
    "    def getReward(self, p):\n",
    "        grid = self.qNet.gw.getItem(p)\n",
    "        if grid == MOUSE:\n",
    "            reward = 1000\n",
    "            end = True\n",
    "        # elif grid == DOG:\n",
    "        #     reward = -100\n",
    "        #     end = True\n",
    "        #     self.qNet.gw.setCatP(p)\n",
    "        elif grid == EMPTY:\n",
    "            reward = -1\n",
    "            end = False\n",
    "        elif grid == CAT:\n",
    "            reward = -1 # (maybe less than reward of empty)\n",
    "            end = False\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown grid item {grid}\")\n",
    "        return reward, end\n",
    "\n",
    "    def act(self, state, action):\n",
    "        p = self.newPosition(state, action)\n",
    "        reward, end = self.getReward(p)\n",
    "        return p, reward, end\n",
    "    \n",
    "    def updateQtable(self, action, p, reward, state):\n",
    "        pqv = self.qNet.calculateQvalue(action, p, reward, state)\n",
    "        self.qNet.updateQtable(pqv, action)\n",
    "\n",
    "    def setTraining(self, training):\n",
    "        self.Training = training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pet school\n",
    "class PetSchool:\n",
    "    def __init__(self, cat:Cat, numEpisodes, maxEpisodeSteps, training=True, minAlpha = 0.02, eps = 0.2):\n",
    "        self.cat = cat\n",
    "        self.training = training\n",
    "        self.NUM_EPISODES = numEpisodes\n",
    "        self.MAX_EPISODE_STEPS = maxEpisodeSteps\n",
    "        self.alphas = np.linspace(1.0, minAlpha, self.NUM_EPISODES)\n",
    "        self.eps = eps\n",
    "\n",
    "    def train(self):\n",
    "        counter = 0\n",
    "        rd = True\n",
    "        for e in range(self.NUM_EPISODES): #  episode: a rund for agent\n",
    "            print(\"episode: \", e)\n",
    "            if e > int(self.NUM_EPISODES/2):\n",
    "                rd = False\n",
    "            state = self.cat.qNet.gw.initCatState(rd=rd)\n",
    "            self.cat.qNet.setAlpha(self.alphas[e])\n",
    "            total_reward  = 0\n",
    "            step = 0\n",
    "            end = False\n",
    "            for _ in range(self.MAX_EPISODE_STEPS): # step: a time step for agent\n",
    "                action = self.cat.qNet.selectAction(deepcopy(state), self.training)\n",
    "                p, reward, end = self.cat.act(state, action)\n",
    "                self.catMoveTo(p)\n",
    "                if not end: # stupid mouse may move to cat\n",
    "                    end = self.mouseMove(self.cat.qNet.gw.getMouseP())\n",
    "                    total_reward += 1000 # not update qtable\n",
    "                total_reward += reward\n",
    "                step += 1\n",
    "                counter += 1\n",
    "                if end:\n",
    "                    print(\"catch the mouse!!!\")\n",
    "                    print(\"total reward: \", total_reward, \"steps: \", step)\n",
    "                    break\n",
    "        print(\"counter: \", counter)\n",
    "    \n",
    "    def catMoveTo(self, p):\n",
    "        self.cat.qNet.gw.setCatP(p)\n",
    "\n",
    "    def show(self):\n",
    "        self.cat.qNet.gw.show()\n",
    "        print(\"qTable: \", self.cat.qNet.qt)\n",
    "        print(\"\\nparams: \", self.cat.qNet.rets)\n",
    "\n",
    "    def initqTable(self, actions, size):\n",
    "        d = {}\n",
    "        for i in range(size[0]):\n",
    "            for j in range(size[1]):\n",
    "                d[i,j] = np.zeros(len(actions))\n",
    "        return d\n",
    "    \n",
    "    # @Daniel-Molpe\n",
    "    def mouseMove(self, oldPos, p=0.5): # goal (mouse) moves randomly with prob p every time the cat moves\n",
    "        side = min(self.cat.qNet.gw.getNumColumns(), self.cat.qNet.gw.getNumRows()) # Number of cells per side of the grid\n",
    "        end = False\n",
    "        if np.random.random() < p:\n",
    "            n = np.random.random()\n",
    "            if n < 0.25:\n",
    "                newPos = (max(0, oldPos[0]-1), oldPos[1])\n",
    "            elif n < 0.5:\n",
    "                newPos = (min(side - 1, oldPos[0]+1),oldPos[1])\n",
    "            elif n < 0.75:\n",
    "                newPos = (oldPos[0], max(0, oldPos[1]-1))\n",
    "            else:\n",
    "                newPos = (oldPos[0], min(side - 1, oldPos[1]+1))\n",
    "        else:\n",
    "            newPos = oldPos\n",
    "        self.cat.qNet.gw.setMouseP(newPos)\n",
    "        if self.cat.qNet.gw.getCatP == newPos: # mouse ends the training\n",
    "            end = True\n",
    "        return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super parameter\n",
    "gridSize = [3, 3]\n",
    "catP = [gridSize[0]-1, gridSize[0]-1]\n",
    "mouseP = [0, 0]\n",
    "EPS = 20\n",
    "MAX_EPS_STEP = 30\n",
    "sizeOfParams = 9\n",
    "gamma = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0\n",
      "catch the mouse!!!\n",
      "total reward:  1000 steps:  1\n",
      "episode:  1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "getMouseP() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28728/428452130.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mpetSchool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPetSchool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPS_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# start training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mpetSchool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;31m# show what have been learned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mpetSchool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28728/2539965652.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatMoveTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# stupid mouse may move to cat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmouseMove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMouseP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMouseP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;31m# not update qtable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getMouseP() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "def initqTable(size, actions=[UP, DOWN, LEFT, RIGHT]):\n",
    "    d = {}\n",
    "    for i in range(size[0]):\n",
    "        for j in range(size[1]):\n",
    "            for m in range(size[0]):\n",
    "                for n in range(size[1]):\n",
    "                    d[i, j, m, n] = np.zeros(len(actions))\n",
    "    return d\n",
    "\n",
    "# initGridWorld\n",
    "gridWorld = GridWorld(gridSize, catP=catP, mouseP=mouseP)\n",
    "# init q Table\n",
    "qt = initqTable(gridSize, actions=ACTIONS)\n",
    "# init q Circuit\n",
    "qNet = QNet(qt, gridWorld, gamma=gamma, actions=ACTIONS)\n",
    "# init cat\n",
    "cat = Cat(qNet=qNet)\n",
    "# init pet school\n",
    "petSchool = PetSchool(cat, EPS, MAX_EPS_STEP)\n",
    "# start training\n",
    "petSchool.train()\n",
    "# show what have been learned\n",
    "petSchool.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d86c190dfcadcdaa67edec4a1ea82702241987b5b1f320c920d3d4ca36fee5b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

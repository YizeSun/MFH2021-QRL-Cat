{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT = \"c\"\n",
    "DOG = \"d\"\n",
    "MOUSE = \"m\"\n",
    "EMPTY = \"emp\"\n",
    "\n",
    "field = [[MOUSE, EMPTY],\n",
    "         [DOG, CAT]]\n",
    "\n",
    "def showField():\n",
    "    for row in field:\n",
    "        print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m emp\n",
      "d c\n"
     ]
    }
   ],
   "source": [
    "showField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the _ _name_ _ variable used?**\n",
    "\n",
    "The __name__ variable (two underscores before and after) is a special Python variable. It gets its value depending on how we execute the containing script.\n",
    "\n",
    "Sometimes you write a script with functions that might be useful in other scripts as well. In Python, you can import that script as a module in another script.\n",
    "\n",
    "Thanks to this special variable, you can decide whether you want to run the script. Or that you want to import the functions defined in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4512076517437852602\n",
      "False\n",
      "[['m', 'emp'], ['d', 'c']]\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "class State:\n",
    "    def __init__(self, field, catP):\n",
    "        self.field = field\n",
    "        self.catP = catP #Cat position\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, State) and self.field == other.field and self.catP == other.catP\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(str(self.field) + str(self.catP))\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"State(feld={self.field}, cat_pos={self.catP}\" \n",
    "\n",
    "# actions:\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "\n",
    "ACTIONS = [UP, DOWN, LEFT, RIGHT]\n",
    "\n",
    "iniState = State(field=field, catP=[1, 1])\n",
    "\n",
    "print(iniState.__hash__())\n",
    "print(iniState.__eq__(5))\n",
    "newField = deepcopy(iniState.field)\n",
    "print(newField)\n",
    "print(iniState.catP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose action to do\n",
    "def act(state, action):\n",
    "    \n",
    "    def newCatPosition(state, action):\n",
    "        p = deepcopy(state.catP)\n",
    "        if action == UP:\n",
    "            p[0] = max(0, p[0] - 1)\n",
    "        elif action == DOWN:\n",
    "            p[0] = min(len(state.field) - 1, p[0]+1)\n",
    "        elif action == LEFT:\n",
    "            p[1] = max(0, p[1] - 1)\n",
    "        elif action == RIGHT:\n",
    "            p[1] = min(len(state.field) - 1, p[1] + 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unkown action {action}\")\n",
    "\n",
    "        return p\n",
    "\n",
    "\n",
    "    p = newCatPosition(state, action)\n",
    "    fieldItem = state.field[p[0]][p[1]] #Define who lives in the new cat position \n",
    "    newField = deepcopy(state.field)\n",
    "\n",
    "    if fieldItem == DOG:\n",
    "        reward = -100\n",
    "        isDone = True # The dog has killed the cat\n",
    "        newField[p[0]][p[1]] += CAT\n",
    "    elif fieldItem == MOUSE:\n",
    "        reward = 1000\n",
    "        isDone = True # The cat has eaten the mouse\n",
    "        newField[p[0]][p[1]] += CAT\n",
    "    elif fieldItem == EMPTY:\n",
    "        reward = -1\n",
    "        isDone = False # Nothing has changed\n",
    "        old = state.catP\n",
    "        newField[old[0]][old[1]] = EMPTY\n",
    "        newField[p[0]][p[1]] = CAT\n",
    "    elif fieldItem == CAT:\n",
    "        reward = -1\n",
    "        isDone = False # Nothing has changed\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown field item {fieldItem}\")\n",
    "    \n",
    "    return State(field=newField, catP=p), reward, isDone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.94842105, 0.89684211, 0.84526316, 0.79368421,\n",
       "       0.74210526, 0.69052632, 0.63894737, 0.58736842, 0.53578947,\n",
       "       0.48421053, 0.43263158, 0.38105263, 0.32947368, 0.27789474,\n",
       "       0.22631579, 0.17473684, 0.12315789, 0.07157895, 0.02      ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just an example of how np.linspace works\n",
    "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) # for reproducibility\n",
    "\n",
    "N_STATES = 4\n",
    "N_EPISODES = 20\n",
    "\n",
    "MAX_EPISODE_STEPS = 100\n",
    "\n",
    "MIN_ALPHA = 0.02\n",
    "\n",
    "alphas = np.linspace(1.0, MIN_ALPHA, N_EPISODES)\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "\n",
    "q_table = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellmann(state, action = None):\n",
    "    if state not in q_table:\n",
    "        q_table[state] = np.zeros(len(ACTIONS)) # the step number is a key, the value is an array of actions\n",
    "    if action is None:\n",
    "        return q_table[state]\n",
    "    return q_table[state][action]\n",
    "\n",
    "def selectAction(state): # why do we need that?\n",
    "    if random.uniform(0, 1) < eps:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return np.argmax(bellmann(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up=0.0, down=0.0, left=0.0, right=0.0\n",
      "Episode 1: total reward -> 999, steps: 2\n",
      "Episode 2: total reward -> 998, steps: 3\n",
      "Episode 3: total reward -> 997, steps: 4\n",
      "Episode 4: total reward -> 997, steps: 4\n",
      "Episode 5: total reward -> 999, steps: 2\n",
      "Episode 6: total reward -> 999, steps: 2\n",
      "Episode 7: total reward -> 998, steps: 3\n",
      "Episode 8: total reward -> -100, steps: 1\n",
      "Episode 9: total reward -> -101, steps: 2\n",
      "Episode 10: total reward -> 999, steps: 2\n",
      "Episode 11: total reward -> 999, steps: 2\n",
      "Episode 12: total reward -> 999, steps: 2\n",
      "Episode 13: total reward -> 999, steps: 2\n",
      "Episode 14: total reward -> 999, steps: 2\n",
      "Episode 15: total reward -> 999, steps: 2\n",
      "Episode 16: total reward -> 998, steps: 3\n",
      "Episode 17: total reward -> 999, steps: 2\n",
      "Episode 18: total reward -> 999, steps: 2\n",
      "Episode 19: total reward -> 999, steps: 2\n",
      "Episode 20: total reward -> 999, steps: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "r = bellmann(iniState)\n",
    "print(f\"up={r[UP]}, down={r[DOWN]}, left={r[LEFT]}, right={r[RIGHT]}\")\n",
    "\n",
    "\n",
    "for e in range(N_EPISODES):\n",
    "    state = iniState\n",
    "    total_reward = 0\n",
    "    alpha = alphas[e]\n",
    "    counter = 0\n",
    "    for _ in range(MAX_EPISODE_STEPS):\n",
    "        action = selectAction(state)\n",
    "        next_state, reward, done = act(state, action) # we know the next state, \n",
    "        # all we need is to set new values in Q-table\n",
    "        total_reward += reward\n",
    "        \n",
    "        bellmann(state)[action] = bellmann(state, action) + \\\n",
    "                alpha * (reward + gamma *  np.max(bellmann(next_state)) - bellmann(state, action))\n",
    "        state = next_state\n",
    "        counter += 1\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"Episode {e + 1}: total reward -> {total_reward}, steps: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
